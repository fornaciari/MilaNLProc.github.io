[{"authors":["bianchi_federico"],"categories":null,"content":"Federico Bianchi is a Postdoctoral Researcher at Bocconi, where he works on Natural Language Processing. His interests also revolve around Knowledge Representation and Neural-Symbolic Learning and Reasoning. AI Enthusiast, he also works with Knowledge Graphs and Knowledge Graphs Embeddings.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7d7484d2d77b0eb52c213a897b092233","permalink":"https://MilaNLProc.github.io/authors/bianchi_federico/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bianchi_federico/","section":"authors","summary":"Federico Bianchi is a Postdoctoral Researcher at Bocconi, where he works on Natural Language Processing. His interests also revolve around Knowledge Representation and Neural-Symbolic Learning and Reasoning. AI Enthusiast, he also works with Knowledge Graphs and Knowledge Graphs Embeddings.","tags":null,"title":"Federico Bianchi","type":"authors"},{"authors":["hovy_dirk"],"categories":null,"content":"Dirk Hovy is the scientific director of DMI and an Associate Professor in the Marketing Department. He has a background in Computer Science. His research focuses on natural language processing and computational social science, specifically, the interaction of demographic factors and language, and their consequences for performance, fairness, and personalization of statistical models. He is also interested in deep learning, graphical models, and ethical questions of bias and algorithmic fairness in machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a8a1fc48488f967e1eae6643e57018ef","permalink":"https://MilaNLProc.github.io/authors/hovy_dirk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hovy_dirk/","section":"authors","summary":"Dirk Hovy is the scientific director of DMI and an Associate Professor in the Marketing Department. He has a background in Computer Science. His research focuses on natural language processing and computational social science, specifically, the interaction of demographic factors and language, and their consequences for performance, fairness, and personalization of statistical models.","tags":null,"title":"Dirk Hovy","type":"authors"},{"authors":["nozza_debora"],"categories":null,"content":"Debora Nozza is a Postdoctoral Research Fellow at Bocconi University. Her research interests mainly focus on Natural Language Processing, specifically on the detection and counter-acting of hate speech and algorithmic bias on Social Media data in multilingual context. She was Area Chair at the 14th Women in Machine Learning Workshop (WiML) at NeurIPS 2019 and active member of the Milan Women in Machine Learning and Data Science community. She was one of the organizers of the task on Automatic Misogyny Identification (AMI) at Evalita 2018, and one of the organizers of the HatEval Task 5 at SemEval 2019 on multilingual detection of hate speech against immigrants and women in Twitter.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e269dc3af085d59861bfc2cca0f07e28","permalink":"https://MilaNLProc.github.io/authors/nozza_debora/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/nozza_debora/","section":"authors","summary":"Debora Nozza is a Postdoctoral Research Fellow at Bocconi University. Her research interests mainly focus on Natural Language Processing, specifically on the detection and counter-acting of hate speech and algorithmic bias on Social Media data in multilingual context.","tags":null,"title":"Debora Nozza","type":"authors"},{"authors":["Federico Bianchi","Silvia Terragni","Dirk Hovy","Debora Nozza","Elisabetta Fersini"],"categories":[],"content":"","date":1586995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586995200,"objectID":"9470a9b5000cd0a8bfc760a377937070","permalink":"https://MilaNLProc.github.io/publication/2020_crosslingual_topic_model/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2020_crosslingual_topic_model/","section":"publication","summary":"Many data sets in a domain (reviews, forums, news, etc.) exist in parallel languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model, i.e., our model learns topics on one language (here, English), and predicts them for documents in other languages. By using the text of the same document in different languages, we can evaluate the quality of the predictions. Our results show that topics are coherent and stable across languages, which suggests exciting future research directions.","tags":["multilingual","BERT","Topic Model","Representation learning","NLP"],"title":"Cross-lingual Contextualized Topic Models with Zero-shot Learning","type":"publication"},{"authors":["Debora Nozza","Federico Bianchi","Dirk Hovy"],"categories":[],"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"9ded9b5d728733baae303b76b3f9c388","permalink":"https://MilaNLProc.github.io/publication/2020_bertlang/","publishdate":"2020-02-29T14:48:20+01:00","relpermalink":"/publication/2020_bertlang/","section":"publication","summary":"Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at [https://bertlang.unibocconi.it](https://bertlang.unibocconi.it/).","tags":["multilingual","BERT","Representation learning","NLP"],"title":"What the [MASK]? Making Sense of Language-Specific BERT Models","type":"publication"},{"authors":null,"categories":["demographic"],"content":"Dirk Hovy, scientific director of DMI and Associate Professor of computer science, has won an ERC starting grant of 1.5mln euros. His project introduces demographic factors into language processing systems, which will improve algorithmic performance, avoid racism, sexism, and ageism, and open up new applications. What if I wrote that “winning an ERC Grant, Dirk Hovy got a sick result?”. Those familiar with the use of “sick” as a synonym for “great” or “awesome” among teenagers would think that Bocconi Knowledge hired a very young writer (or someone posing as such). The rest would think I went crazy. Current artificial intelligence-based language systems wouldn’t have a clue. “Natural language processing (NLP) technologies,” Prof. Hovy says, “fail to account for demographics both in understanding language and in generating it. And this failure prevents us from reaching human-like performance. It limits possible future applications and it introduces systematic bias against underrepresented demographic groups”.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"d6b57eefc5eca031cd0bb3edb943a34f","permalink":"https://MilaNLProc.github.io/project/integrator/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/integrator/","section":"project","summary":"Incorporating Demographic Factors into Natural Language Processing Models","tags":["demographic","nlp"],"title":"INTEGRATOR","type":"project"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://MilaNLProc.github.io/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"About / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"be566fdb6f0fa08cfea50d77a89a6b5a","permalink":"https://MilaNLProc.github.io/data/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/data/","section":"","summary":"","tags":null,"title":"How to partecipate","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://MilaNLProc.github.io/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"}]